{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holdout validation is actually a specific example of a larger class of validation techniques called k-fold cross-validation. K-fold cross-validation works by:\n",
    "\n",
    "    splitting the full dataset into k equal length partitions,\n",
    "        selecting k-1 partitions as the training set and\n",
    "        selecting the remaining partition as the test set\n",
    "    training the model on the training set,\n",
    "    using the trained model to predict labels on the test set,\n",
    "    computing an error metric (e.g. simple accuracy) and setting aside the value for later,\n",
    "    repeating all of the above steps k-1 times, until each partition has been used as the test set for an iteration,\n",
    "    calculating the mean of the k error values.\n",
    "\n",
    "Using 5 or 10 folds is common for k-fold cross-validation. Here's a diagram describing each iteration of 5-fold cross validation:\n",
    "\n",
    "Since you're training k models, the more number of folds you use the longer it takes. When working with large datasets, often only a few number of folds are used because of the time and cost it takes, with the tradeoff that having more training examples helps improve the accuracy even with less folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index       gpa         gre  actual_label  fold\n",
      "0     90  3.498628  632.133663             0     1\n",
      "1    109  3.285823  528.973583             0     1\n",
      "2     99  2.916654  661.170680             0     1\n",
      "3    123  3.119102  548.824829             0     1\n",
      "4    629  2.944457  624.275140             1     1\n",
      "     index       gpa         gre  actual_label  fold\n",
      "639    380  2.894841  555.032255             0     5\n",
      "640    254  3.108220  617.103330             0     5\n",
      "641    223  3.364396  624.524777             0     5\n",
      "642    371  2.746944  529.966555             0     5\n",
      "643    144  2.687906  540.035697             0     5\n"
     ]
    }
   ],
   "source": [
    "#partioning dataset \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "admissions = pd.read_csv(\"admissions.csv\")\n",
    "admissions[\"actual_label\"] = admissions[\"admit\"]\n",
    "admissions = admissions.drop(\"admit\", axis=1)\n",
    "\n",
    "shuffled_index = np.random.permutation(admissions.index)\n",
    "shuffled_admissions = admissions.loc[shuffled_index]\n",
    "admissions = shuffled_admissions.reset_index()\n",
    "\n",
    "admissions.ix[0:128, \"fold\"] = 1\n",
    "admissions.ix[129:257, \"fold\"] = 2\n",
    "admissions.ix[258:386, \"fold\"] = 3\n",
    "admissions.ix[387:514, \"fold\"] = 4\n",
    "admissions.ix[515:644, \"fold\"] = 5\n",
    "# Ensure the column is set to integer type.\n",
    "admissions[\"fold\"] = admissions[\"fold\"].astype('int')\n",
    "\n",
    "\n",
    "print(admissions.head())\n",
    "print(admissions.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working in a production environment however, you should use scikit-learn. Scikit-learn has a few different tools that make performing cross validation easy. Similar to having to instantiate a LinearRegression or LogisticRegression object before you can train one of those models, you need to instantiate a KFold class before you can perform k-fold cross-validation:\n",
    "\n",
    "kf = KFold(n, n_folds, shuffle=False, random_state=None)\n",
    "\n",
    "where:\n",
    "\n",
    "    n is the number of observations in the dataset,\n",
    "    n_folds is the number of folds you want to use,\n",
    "    shuffle is used to toggle shuffling of the ordering of the observations in the dataset,\n",
    "    random_state is used to specify a seed value if shuffle is set to True.\n",
    "\n",
    "You'll notice here that only the first parameter depends on the dataset at all. This is because the KFold class returns an iterator object but won't actually handle the training and testing of models. If we're primarily only interested in accuracy and error metrics for each fold, we can use the KFold class in conjunction with the cross_val_score function, which will handle training and testing of the models in each fold.\n",
    "\n",
    "Here are the relevant parameters for the cross_val_score function:\n",
    "\n",
    "cross_val_score(estimator, X, Y, scoring=None, cv=None)\n",
    "\n",
    "where:\n",
    "\n",
    "    estimator is a sklearn model that implements the fit method (e.g. instance of LinearRegression or LogisticRegression),\n",
    "    X is the list or 2D array containing the features you want to train on,\n",
    "    y is a list containing the values you want to predict (target column),\n",
    "    scoring is a string describing the scoring criteria (list of accepted values here).\n",
    "    cv describes the number of folds. Here are some examples of accepted values:\n",
    "        an instance of the KFold class,\n",
    "        an integer representing the number of folds.\n",
    "\n",
    "Depending on the scoring criteria you specify, either a single value is returned (e.g. average_precision) or an array of values (e.g. accuracy), one value for each fold.\n",
    "\n",
    "Here's the general workflow for performing k-fold cross-validation using the classes we just described:\n",
    "\n",
    "    instantiate the model class you want to fit (e.g. LogisticRegression),\n",
    "    instantiate the KFold class and using the parameters to specify the k-fold cross-validation attributes you want,\n",
    "    use the cross_val_score function to return the scoring metric you're interested in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6124031   0.65891473  0.64341085  0.6744186   0.6328125 ]\n",
      "0.644391957364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "admissions = pd.read_csv(\"admissions.csv\")\n",
    "admissions[\"actual_label\"] = admissions[\"admit\"]\n",
    "admissions = admissions.drop(\"admit\", axis=1)\n",
    "kf=KFold(n=admissions.shape[0],n_folds=5,shuffle=True,random_state=8)\n",
    "lr=LogisticRegression()\n",
    "accuracies=cross_val_score(estimator=lr,X=admissions[[\"gpa\"]],y=admissions[\"actual_label\"],cv=kf)\n",
    "print(accuracies)\n",
    "print(accuracies.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "Using 5-fold cross-validation, we achieved an average accuracy score of 64.4%, which closely matches the 63.6% accuracy score we achieved using holdout validation. When working with simple univariate models, often holdout validation is more than enough and the similar accuracy scores confirm this. When you're using multiple features to train a model (multivariate models), performing k-fold cross-validation can give you a better sense of the accuracy you should expect when you use the model on data it wasn't trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
